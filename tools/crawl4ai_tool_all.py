import asyncio
import base64
import io
import gc
import psutil
import time
import json
from typing import Dict, Any, List, Optional, Literal
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler
from crawl4ai import CrawlerRunConfig, CacheMode
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy, DFSDeepCrawlStrategy, BestFirstCrawlingStrategy
from crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter, DomainFilter, ContentTypeFilter
from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter
import logging
from PIL import Image

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# 1. æ‰©å±•è¾“å…¥æ¨¡å‹ä»¥æ”¯æŒæ–°åŠŸèƒ½
class ScrapeParams(BaseModel):
    url: str = Field(description="The URL of the page to scrape.")
    format: Literal['markdown', 'html', 'text'] = Field(default='markdown', description="Output format.")
    css_selector: Optional[str] = Field(default=None, description="CSS selector to extract specific content.")
    include_links: bool = Field(default=True, description="Whether to include links in the output.")
    include_images: bool = Field(default=True, description="Whether to include images in the output.")
    return_screenshot: bool = Field(default=False, description="Whether to return screenshot as base64.")
    return_pdf: bool = Field(default=False, description="Whether to return PDF as base64.")
    screenshot_quality: int = Field(default=70, ge=10, le=100, description="JPEG quality for screenshot (10-100).")
    screenshot_max_width: int = Field(default=1920, description="Maximum width for screenshot.")
    word_count_threshold: int = Field(default=10, description="Minimum words per content block.")
    exclude_external_links: bool = Field(default=True, description="Remove external links from content.")

class CrawlParams(BaseModel):
    url: str = Field(description="The starting URL for the crawl.")
    max_pages: int = Field(default=10, description="Maximum number of pages to crawl.")
    same_domain: bool = Field(default=True, description="Whether to only crawl same domain URLs.")
    depth: int = Field(default=2, description="Crawl depth.")
    strategy: Literal['bfs', 'dfs', 'best_first'] = Field(default='bfs', description="Crawl strategy.")
    include_external: bool = Field(default=False, description="Include external domains.")
    stream_results: bool = Field(default=False, description="Stream results as they complete.")

class DeepCrawlParams(BaseModel):
    url: str = Field(description="The starting URL for deep crawl.")
    max_depth: int = Field(default=2, description="Maximum crawl depth.")
    max_pages: int = Field(default=50, description="Maximum pages to crawl.")
    strategy: Literal['bfs', 'dfs', 'best_first'] = Field(default='bfs', description="Crawl strategy.")
    include_external: bool = Field(default=False, description="Follow external links.")
    keywords: Optional[List[str]] = Field(default=None, description="Keywords for relevance scoring.")
    url_patterns: Optional[List[str]] = Field(default=None, description="URL patterns to include.")
    stream: bool = Field(default=False, description="Stream results progressively.")

class ExtractParams(BaseModel):
    url: str = Field(description="The URL to extract structured data from.")
    schema_definition: Dict[str, Any] = Field(description="JSON schema for data extraction.")
    css_selector: Optional[str] = Field(default=None, description="Base CSS selector for extraction.")
    extraction_type: Literal['css', 'llm'] = Field(default='css', description="Extraction strategy type.")
    prompt: Optional[str] = Field(default=None, description="Prompt for LLM extraction.")

class BatchCrawlParams(BaseModel):
    urls: List[str] = Field(description="List of URLs to crawl.")
    stream: bool = Field(default=False, description="Stream results as they complete.")
    concurrent_limit: int = Field(default=3, description="Maximum concurrent crawls.")

class PdfExportParams(BaseModel):
    url: str = Field(description="The URL to export as PDF.")
    return_as_base64: bool = Field(default=True, description="Return PDF as base64 string.")

class ScreenshotParams(BaseModel):
    url: str = Field(description="The URL to capture screenshot.")
    full_page: bool = Field(default=True, description="Whether to capture full page.")
    return_as_base64: bool = Field(default=True, description="Return screenshot as base64 string.")
    quality: int = Field(default=70, ge=10, le=100, description="JPEG quality for screenshot (10-100).")
    max_width: int = Field(default=1920, description="Maximum width for screenshot.")
    max_height: int = Field(default=5000, description="Maximum height for screenshot.")

# 2. æ‰©å±•æ€»çš„å·¥å…·è¾“å…¥æ¨¡å‹
class Crawl4AIInput(BaseModel):
    mode: Literal['scrape', 'crawl', 'deep_crawl', 'extract', 'batch_crawl', 'pdf_export', 'screenshot'] = Field(
        description="The Crawl4AI function to execute."
    )
    parameters: Dict[str, Any] = Field(
        description="Parameters for the selected mode, matching the respective schema."
    )

class ScreenshotCompressor:
    """æˆªå›¾å‹ç¼©å™¨"""
    
    @staticmethod
    def compress_screenshot(base64_data: str, quality: int = 70, max_width: int = 1920, max_height: int = 5000) -> str:
        """å‹ç¼©base64æ ¼å¼çš„æˆªå›¾"""
        try:
            image_data = base64.b64decode(base64_data)
            
            with Image.open(io.BytesIO(image_data)) as img:
                original_format = img.format
                original_size = img.size
                
                if img.size[0] > max_width or img.size[1] > max_height:
                    img.thumbnail((max_width, max_height), Image.Resampling.LANCZOS)
                    logger.info(f"Resized screenshot from {original_size} to {img.size}")
                
                if img.mode in ('RGBA', 'LA'):
                    background = Image.new('RGB', img.size, (255, 255, 255))
                    background.paste(img, mask=img.split()[-1])
                    img = background
                elif img.mode != 'RGB':
                    img = img.convert('RGB')
                
                output_buffer = io.BytesIO()
                img.save(output_buffer, format='JPEG', quality=quality, optimize=True)
                compressed_data = output_buffer.getvalue()
                
                compressed_base64 = base64.b64encode(compressed_data).decode('utf-8')
                
                original_size_kb = len(image_data) // 1024
                compressed_size_kb = len(compressed_data) // 1024
                compression_ratio = (1 - len(compressed_data) / len(image_data)) * 100
                
                logger.info(f"Screenshot compressed: {original_size_kb}KB -> {compressed_size_kb}KB "
                           f"({compression_ratio:.1f}% reduction)")
                
                return compressed_base64
                
        except Exception as e:
            logger.error(f"Screenshot compression failed: {str(e)}")
            return base64_data

    @staticmethod
    def get_screenshot_info(base64_data: str) -> Dict[str, Any]:
        """è·å–æˆªå›¾ä¿¡æ¯"""
        try:
            image_data = base64.b64decode(base64_data)
            with Image.open(io.BytesIO(image_data)) as img:
                return {
                    "format": img.format,
                    "size": img.size,
                    "mode": img.mode,
                    "data_size_kb": len(image_data) // 1024
                }
        except Exception as e:
            logger.error(f"Failed to get screenshot info: {str(e)}")
            return {"error": str(e)}

# 3. ä¼˜åŒ–å†…å­˜ç®¡ç†çš„ Crawl4AI å·¥å…·ç±»
class EnhancedCrawl4AITool:
    name = "crawl4ai"
    description = (
        "A powerful open-source tool to scrape, crawl, extract structured data, export PDFs, and capture screenshots from web pages. "
        "Supports deep crawling with multiple strategies (BFS, DFS, BestFirst), batch URL processing, AI-powered extraction, "
        "and advanced content filtering. All outputs are returned as memory streams (base64 for binary data)."
    )
    input_schema = Crawl4AIInput

    def __init__(self):
        self.crawler = None
        self._initialized = False
        self._task_count = 0
        self._cleanup_interval = 5  # å»¶é•¿æ¸…ç†é—´éš”
        self._memory_threshold = 80  # æé«˜å†…å­˜é˜ˆå€¼
        self._max_memory_mb = 1500   # å¢åŠ å†…å­˜é™åˆ¶
        self._browser_start_time = None
        self._max_browser_uptime = 1200
        self._last_memory_check = 0
        self._memory_check_interval = 60
        self._browser_lock = asyncio.Lock()
        self.compressor = ScreenshotCompressor()
        logger.info("EnhancedCrawl4AITool instance created")

    async def _check_memory_health(self) -> bool:
        """æ£€æŸ¥ç³»ç»Ÿå†…å­˜å¥åº·çŠ¶æ€ - ä¼˜åŒ–ç‰ˆæœ¬"""
        current_time = time.time()
        
        # å‡å°‘å†…å­˜æ£€æŸ¥é¢‘ç‡
        if current_time - self._last_memory_check < self._memory_check_interval:
            return True
            
        self._last_memory_check = current_time
        
        try:
            memory = psutil.virtual_memory()
            process = psutil.Process()
            process_memory_mb = process.memory_info().rss / 1024 / 1024
            
            logger.info(f"å†…å­˜çŠ¶æ€ - ç³»ç»Ÿ: {memory.percent}%, è¿›ç¨‹: {process_memory_mb:.1f}MB")
            
            # åªæœ‰åœ¨å†…å­˜ä½¿ç”¨ç‡éå¸¸é«˜æ—¶æ‰è¿›è¡Œæ¸…ç†
            if memory.percent > 95:  # ç´§æ€¥æƒ…å†µé˜ˆå€¼
                logger.warning(f"âš ï¸ ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory.percent}%")
                return False
                
            if process_memory_mb > self._max_memory_mb:
                logger.warning(f"âš ï¸ è¿›ç¨‹å†…å­˜ä½¿ç”¨è¿‡é«˜: {process_memory_mb:.1f}MB")
                return False
                
            if (self._browser_start_time and 
                current_time - self._browser_start_time > self._max_browser_uptime):
                logger.warning("ğŸ•’ æµè§ˆå™¨å®ä¾‹è¿è¡Œæ—¶é—´è¿‡é•¿")
                return False
                
            return True
            
        except Exception as e:
            logger.error(f"å†…å­˜æ£€æŸ¥å¤±è´¥: {str(e)}")
            return True

    async def _get_system_memory_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯"""
        try:
            memory = psutil.virtual_memory()
            process = psutil.Process()
            return {
                "system_memory_percent": memory.percent,
                "system_memory_used_mb": memory.used / 1024 / 1024,
                "system_memory_total_mb": memory.total / 1024 / 1024,
                "process_memory_mb": process.memory_info().rss / 1024 / 1024,
                "browser_uptime_seconds": time.time() - self._browser_start_time if self._browser_start_time else 0
            }
        except Exception as e:
            logger.error(f"è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e)}

    async def initialize(self):
        """åˆå§‹åŒ–æµè§ˆå™¨å®ä¾‹"""
        async with self._browser_lock:
            if not self._initialized:
                logger.info("ğŸš€ åˆå§‹åŒ– crawl4ai æµè§ˆå™¨...")
                await self._create_crawler()
                self._initialized = True
                logger.info("âœ… crawl4ai æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")

    async def _create_crawler(self):
        """åˆ›å»ºæ–°çš„çˆ¬è™«å®ä¾‹"""
        logger.info("ğŸ†• åˆ›å»ºæ–°çš„ AsyncWebCrawler å®ä¾‹...")
        try:
            self.crawler = AsyncWebCrawler(
                browser_type="chromium",
                headless=True,
                verbose=False,
                browser_args=[
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-accelerated-2d-canvas',
                    '--no-first-run',
                    '--no-zygote',
                    '--single-process',
                    '--disable-gpu',
                    '--memory-pressure-off',
                    '--window-size=1280,720'
                ]
            )
            await self.crawler.__aenter__()
            self._browser_start_time = time.time()
            logger.info("âœ… AsyncWebCrawler å®ä¾‹åˆ›å»ºå¹¶å¯åŠ¨")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºçˆ¬è™«å®ä¾‹å¤±è´¥: {e}")
            self.crawler = None
            raise

    async def _get_crawler(self):
        """è·å–çˆ¬è™«å®ä¾‹ï¼Œç¡®ä¿ä¸ä¸ºNone"""
        async with self._browser_lock:
            if self.crawler is None:
                logger.warning("ğŸ”„ çˆ¬è™«å®ä¾‹ä¸ºNoneï¼Œé‡æ–°åˆ›å»º...")
                await self._create_crawler()
            return self.crawler

    async def _handle_browser_crash(self, error: Exception):
        """å¤„ç†æµè§ˆå™¨å´©æºƒ"""
        logger.error(f"ğŸ”„ æµè§ˆå™¨å´©æºƒï¼Œå°è¯•æ¢å¤: {str(error)}")
        async with self._browser_lock:
            if self.crawler:
                try:
                    await self.crawler.__aexit__(None, None, None)
                except:
                    pass
            self.crawler = None
            self._initialized = False
            self._browser_start_time = None
        
        gc.collect()
        await asyncio.sleep(2)
        
        try:
            await self.initialize()
            logger.info("âœ… æµè§ˆå™¨å´©æºƒæ¢å¤æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æµè§ˆå™¨å´©æºƒæ¢å¤å¤±è´¥: {e}")
            raise

    async def _cleanup_after_task(self):
        """ä»»åŠ¡åæ¸…ç†é¡µé¢èµ„æº - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # âœ… 1. ä¸»åŠ¨æ¸…ç†ï¼šæ¯æ¬¡éƒ½å°è¯•å…³é—­å¤šä½™é¡µé¢
            crawler = await self._get_crawler()
            if crawler and hasattr(crawler, 'browser') and crawler.browser and crawler.browser.is_connected():
                try:
                    pages = await crawler.browser.pages()
                    # ä¿ç•™ç¬¬ä¸€ä¸ªé¡µé¢ï¼ˆé€šå¸¸æ˜¯ about:blankï¼‰ï¼Œå…³é—­å…¶ä»–æ‰€æœ‰é¡µé¢
                    if len(pages) > 1:
                        for page in pages[1:]:
                            await page.close()
                except Exception as e:
                    logger.warning(f"æ¸…ç†é¡µé¢æ—¶å‡ºé”™: {e}")

            # âœ… 2. å®šæœŸæ·±åº¦æ£€æŸ¥ï¼šè¾¾åˆ°æ¸…ç†é—´éš”æ—¶æ‰æ£€æŸ¥å†…å­˜
            if self._task_count % self._cleanup_interval == 0:
                if not await self._check_memory_health():
                    logger.warning("å†…å­˜å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œæ‰§è¡Œå¼ºåˆ¶æ¸…ç†ã€‚")
                    await self._force_memory_cleanup()
            
            gc.collect()
            
        except Exception as e:
            logger.warning(f"ä»»åŠ¡åæ¸…ç†å‡ºç°è­¦å‘Š: {e}")

    async def _force_memory_cleanup(self):
        """å¼ºåˆ¶å†…å­˜æ¸…ç† - é‡å¯æµè§ˆå™¨å®ä¾‹"""
        async with self._browser_lock:
            if self.crawler:
                logger.info("ğŸ”„ æ‰§è¡Œå¼ºåˆ¶å†…å­˜æ¸…ç† - é‡å¯æµè§ˆå™¨å®ä¾‹")
                try:
                    await self.crawler.__aexit__(None, None, None)
                except Exception as e:
                    logger.error(f"å…³é—­æ—§æµè§ˆå™¨å®ä¾‹æ—¶å‡ºé”™: {e}")
                finally:
                    self.crawler = None
                    self._initialized = False
                    self._browser_start_time = None
            
            gc.collect()
            await asyncio.sleep(1) # çŸ­æš‚ç­‰å¾…èµ„æºé‡Šæ”¾
            
            # âœ… 3. é‡å»ºå®ä¾‹ï¼šæ¸…ç†åç«‹å³é‡å»ºï¼Œç¡®ä¿ä¸‹ä¸€æ¬¡è°ƒç”¨å¯ç”¨
            try:
                await self.initialize()
                logger.info("âœ… æµè§ˆå™¨å®ä¾‹é‡å¯å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ é‡å¯æµè§ˆå™¨å®ä¾‹å¤±è´¥: {e}")

    async def _execute_with_timeout(self, coro, timeout: int = 60):
        """å¸¦è¶…æ—¶çš„åç¨‹æ‰§è¡Œ"""
        try:
            return await asyncio.wait_for(coro, timeout=timeout)
        except asyncio.TimeoutError:
            logger.error(f"â° æ“ä½œè¶…æ—¶ ({timeout}ç§’)")
            raise
        except Exception as e:
            logger.error(f"âŒ æ“ä½œæ‰§è¡Œå¤±è´¥: {str(e)}")
            raise

    async def _scrape_single_url(self, params: ScrapeParams) -> Dict[str, Any]:
        """æŠ“å–å•ä¸ªURL - ä½¿ç”¨æ–‡æ¡£æ¨èçš„æœ€ä½³å®è·µ"""
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {
                    "success": False, 
                    "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–",
                    "memory_info": await self._get_system_memory_info()
                }
            
            # ä½¿ç”¨æ–‡æ¡£æ¨èçš„ CrawlerRunConfig é…ç½®
            config_kwargs = {
                "cache_mode": CacheMode.BYPASS,
                "css_selector": params.css_selector,
                "exclude_external_links": params.exclude_external_links,
                "exclude_external_images": not params.include_images,
                "pdf": params.return_pdf,
                "screenshot": params.return_screenshot,
                "word_count_threshold": params.word_count_threshold,
                "remove_overlay_elements": True,
                "process_iframes": True
            }
            
            config = CrawlerRunConfig(**config_kwargs)
            
            logger.info(f"ğŸŒ æŠ“å– URL: {params.url}")
            
            result = await self._execute_with_timeout(
                crawler.arun(url=params.url, config=config),
                timeout=120
            )
            
            # ğŸ¯ æ ¸å¿ƒä¿®å¤ï¼šå¢åŠ å¯¹ç»“æœå’Œå†…å®¹çš„åŒé‡æ£€æŸ¥
            content = getattr(result, 'markdown', '') or getattr(result, 'cleaned_html', '')
            if not result.success or not content.strip():
                error_message = result.error_message or "æŠ“å–æˆåŠŸä½†æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æœ¬å†…å®¹ã€‚"
                logger.error(f"âŒ æŠ“å–å¤±è´¥ {params.url}: {error_message}")
                return {"success": False, "error": f"æŠ“å–å¤±è´¥: {error_message}", "memory_info": await self._get_system_memory_info()}
            
            # æ„å»ºå“åº”æ•°æ®
            output_data = {
                "success": True,
                "url": params.url,
                "content": content, # ä½¿ç”¨å·²æ ¡éªŒçš„å†…å®¹
                "cleaned_html": getattr(result, 'cleaned_html', ''),
                "metadata": {
                    "title": getattr(result, 'title', ''),
                    "description": getattr(result, 'description', ''),
                    "word_count": len(content),
                    "status_code": getattr(result, 'status_code', 200)
                },
                "memory_info": await self._get_system_memory_info()
            }
            
            # æ·»åŠ é“¾æ¥ä¿¡æ¯
            if hasattr(result, 'links'):
                output_data["links"] = {
                    "internal": getattr(result, 'internal_links', []),
                    "external": getattr(result, 'external_links', [])
                }
                
            # æ·»åŠ æˆªå›¾ï¼ˆå¸¦å‹ç¼©ï¼‰
            if params.return_screenshot and hasattr(result, 'screenshot') and result.screenshot:
                compressed_screenshot = self.compressor.compress_screenshot(
                    result.screenshot,
                    quality=params.screenshot_quality,
                    max_width=params.screenshot_max_width
                )
                
                original_info = self.compressor.get_screenshot_info(result.screenshot)
                compressed_info = self.compressor.get_screenshot_info(compressed_screenshot)
                
                output_data["screenshot"] = {
                    "data": compressed_screenshot,
                    "format": "base64",
                    "type": "image/jpeg",
                    "compression_info": {
                        "original": original_info,
                        "compressed": compressed_info
                    }
                }
                
            # æ·»åŠ PDF
            if params.return_pdf and hasattr(result, 'pdf') and result.pdf:
                pdf_base64 = base64.b64encode(result.pdf).decode('utf-8')
                output_data["pdf"] = {
                    "data": pdf_base64,
                    "format": "base64",
                    "type": "application/pdf",
                    "size_bytes": len(result.pdf)
                }
                
            logger.info(f"âœ… æˆåŠŸæŠ“å– {params.url}, å†…å®¹é•¿åº¦: {len(output_data['content'])}")
            return output_data
            
        except asyncio.TimeoutError:
            logger.error(f"â° æŠ“å–æ“ä½œè¶…æ—¶: {params.url}")
            return {
                "success": False, 
                "error": "æŠ“å–æ“ä½œè¶…æ—¶ï¼ˆ120ç§’ï¼‰",
                "memory_info": await self._get_system_memory_info()
            }
        except Exception as e:
            logger.error(f"âŒ _scrape_single_url é”™è¯¯: {str(e)}")
            if "browser" in str(e).lower() or "context" in str(e).lower() or "NoneType" in str(e):
                await self._handle_browser_crash(e)
            return {
                "success": False, 
                "error": f"æŠ“å–é”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }
        finally:
            await self._cleanup_after_task()

    async def _deep_crawl_website(self, params: DeepCrawlParams) -> Dict[str, Any]:
        """æ·±åº¦çˆ¬å–ç½‘ç«™ - åŸºäºæ–‡æ¡£çš„å®Œæ•´å®ç°"""
        logger.info(f"ğŸ•·ï¸ å¼€å§‹æ·±åº¦ç½‘ç«™çˆ¬å–: {params.url}, æ·±åº¦: {params.max_depth}, æœ€å¤§é¡µé¢: {params.max_pages}")
        
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {
                    "success": False, 
                    "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–",
                    "memory_info": await self._get_system_memory_info()
                }
            
            # æ ¹æ®ç­–ç•¥é€‰æ‹©æ·±åº¦çˆ¬å–æ–¹æ³•
            if params.strategy == 'bfs':
                deep_crawl_strategy = BFSDeepCrawlStrategy(
                    max_depth=params.max_depth,
                    include_external=params.include_external,
                    max_pages=params.max_pages
                )
            elif params.strategy == 'dfs':
                deep_crawl_strategy = DFSDeepCrawlStrategy(
                    max_depth=params.max_depth,
                    include_external=params.include_external,
                    max_pages=params.max_pages
                )
            elif params.strategy == 'best_first':
                # ä¸ºBestFirstç­–ç•¥æ·»åŠ å…³é”®è¯è¯„åˆ†å™¨
                scorer = None
                if params.keywords:
                    scorer = KeywordRelevanceScorer(
                        keywords=params.keywords,
                        weight=0.7
                    )
                
                deep_crawl_strategy = BestFirstCrawlingStrategy(
                    max_depth=params.max_depth,
                    include_external=params.include_external,
                    max_pages=params.max_pages,
                    url_scorer=scorer
                )
            else:
                deep_crawl_strategy = BFSDeepCrawlStrategy(
                    max_depth=params.max_depth,
                    include_external=params.include_external,
                    max_pages=params.max_pages
                )
            
            # æ„å»ºè¿‡æ»¤å™¨é“¾ï¼ˆå¦‚æœæä¾›äº†URLæ¨¡å¼ï¼‰
            filter_chain = None
            if params.url_patterns:
                url_filter = URLPatternFilter(patterns=params.url_patterns)
                filter_chain = FilterChain([url_filter])
                deep_crawl_strategy.filter_chain = filter_chain
            
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                deep_crawl_strategy=deep_crawl_strategy,
                scraping_strategy=LXMLWebScrapingStrategy(),
                stream=params.stream,
                verbose=True
            )
            
            crawled_pages = []
            total_pages = 0
            
            if params.stream:
                # æµå¼å¤„ç†ç»“æœ
                async for result in await crawler.arun(params.url, config=config):
                    if result.success:
                        page_data = {
                            "url": result.url,
                            "title": getattr(result, 'title', ''),
                            "content": getattr(result, 'markdown', ''),
                            "depth": result.metadata.get('depth', 0),
                            "score": result.metadata.get('score', 0),
                            "metadata": {
                                "word_count": len(getattr(result, 'markdown', '')),
                            }
                        }
                        crawled_pages.append(page_data)
                        total_pages += 1
            else:
                # æ‰¹é‡å¤„ç†ç»“æœ
                results = await self._execute_with_timeout(
                    crawler.arun(params.url, config=config),
                    timeout=300
                )
                
                for result in results:
                    if hasattr(result, 'success') and result.success:
                        page_data = {
                            "url": result.url,
                            "title": getattr(result, 'title', ''),
                            "content": getattr(result, 'markdown', ''),
                            "depth": result.metadata.get('depth', 0),
                            "score": result.metadata.get('score', 0),
                            "metadata": {
                                "word_count": len(getattr(result, 'markdown', '')),
                            }
                        }
                        crawled_pages.append(page_data)
                        total_pages += 1
            
            return {
                "success": True,
                "crawled_pages": crawled_pages,
                "total_pages": total_pages,
                "summary": {
                    "start_url": params.url,
                    "max_depth": params.max_depth,
                    "strategy": params.strategy,
                    "pages_crawled": total_pages
                },
                "memory_info": await self._get_system_memory_info()
            }
            
        except asyncio.TimeoutError:
            return {
                "success": False, 
                "error": "æ·±åº¦çˆ¬å–æ“ä½œè¶…æ—¶ï¼ˆ300ç§’ï¼‰",
                "memory_info": await self._get_system_memory_info()
            }
        except Exception as e:
            logger.error(f"âŒ æ·±åº¦çˆ¬å–é”™è¯¯: {str(e)}")
            if "browser" in str(e).lower() or "context" in str(e).lower() or "NoneType" in str(e):
                await self._handle_browser_crash(e)
            return {
                "success": False, 
                "error": f"æ·±åº¦çˆ¬å–é”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }
        finally:
            await self._cleanup_after_task()

    async def _batch_crawl_urls(self, params: BatchCrawlParams) -> Dict[str, Any]:
        """æ‰¹é‡çˆ¬å–å¤šä¸ªURL - ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info(f"ğŸ”— å¼€å§‹æ‰¹é‡çˆ¬å– {len(params.urls)} ä¸ªURL")
        
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {
                    "success": False, 
                    "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–",
                    "memory_info": await self._get_system_memory_info()
                }
            
            # ä½¿ç”¨æ›´è½»é‡çš„é…ç½®
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                word_count_threshold=10,
                stream=params.stream
            )
            
            crawled_results = []
            successful_crawls = 0
            
            # å¯¹äºæ‰¹é‡çˆ¬å–ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„æ–¹å¼
            for url in params.urls:
                if len(crawled_results) >= 10:  # å®‰å…¨é™åˆ¶
                    break
                    
                try:
                    result = await self._execute_with_timeout(
                        crawler.arun(url=url, config=config),
                        timeout=60
                    )
                    
                    if result.success:
                        page_data = {
                            "url": result.url,
                            "title": getattr(result, 'title', ''),
                            "content": getattr(result, 'markdown', ''),
                            "metadata": {
                                "word_count": len(getattr(result, 'markdown', '')),
                                "status_code": getattr(result, 'status_code', 200)
                            }
                        }
                        crawled_results.append(page_data)
                        successful_crawls += 1
                    else:
                        crawled_results.append({
                            "url": url,
                            "error": result.error_message,
                            "success": False
                        })
                        
                    # æ¯ä¸ªURLä¹‹é—´çŸ­æš‚å»¶è¿Ÿ
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    crawled_results.append({
                        "url": url,
                        "error": str(e),
                        "success": False
                    })
            
            return {
                "success": True,
                "results": crawled_results,
                "summary": {
                    "total_urls": len(params.urls),
                    "successful_crawls": successful_crawls,
                    "failed_crawls": len(params.urls) - successful_crawls,
                    "success_rate": (successful_crawls / len(params.urls)) * 100 if params.urls else 0
                },
                "memory_info": await self._get_system_memory_info()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡çˆ¬å–é”™è¯¯: {str(e)}")
            if "browser" in str(e).lower() or "context" in str(e).lower() or "NoneType" in str(e):
                await self._handle_browser_crash(e)
            return {
                "success": False, 
                "error": f"æ‰¹é‡çˆ¬å–é”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }
        finally:
            await self._cleanup_after_task()

    async def _extract_structured_data(self, params: ExtractParams) -> Dict[str, Any]:
        """æå–ç»“æ„åŒ–æ•°æ® - æœ€ç»ˆå®Œæ•´ä¿®å¤ç‰ˆ"""
        logger.info(f"ğŸ” ä»é¡µé¢æå–ç»“æ„åŒ–æ•°æ®: {params.url}, ç±»å‹: {params.extraction_type}")
        
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {"success": False, "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–", "memory_info": await self._get_system_memory_info()}
            
            # ğŸ¯ æœ€ç»ˆä¿®å¤ï¼šç¡®ä¿schemaåŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
            schema = params.schema_definition.copy()
            if params.extraction_type == 'css':
                # âœ… 1. ç¡®ä¿æœ‰ baseSelectorï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰
                css_selector = params.css_selector or 'body'
                if 'baseSelector' not in schema:
                    schema['baseSelector'] = css_selector
                    logger.info(f"ğŸ”§ è‡ªåŠ¨æ·»åŠ  baseSelector åˆ° schema: {schema['baseSelector']}")
                
                # âœ… 2. ç¡®ä¿æœ‰ fieldsï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰
                if 'fields' not in schema:
                    schema['fields'] = [
                        {
                            "name": "content",
                            "selector": css_selector,  # âœ… ä½¿ç”¨å®‰å…¨çš„å˜é‡ï¼Œè€Œä¸æ˜¯ schema['baseSelector']
                            "type": "text",
                            "multiple": True
                        }
                    ]
                    logger.info(f"ğŸ”§ è‡ªåŠ¨æ·»åŠ é»˜è®¤ fields åˆ° schema")
                
                # âœ… 3. ç¡®ä¿æœ‰ nameï¼ˆé¢å¤–ä¿éšœï¼‰
                if 'name' not in schema:
                    schema['name'] = "ExtractedData"
                    logger.info(f"ğŸ”§ è‡ªåŠ¨æ·»åŠ  name åˆ° schema")
            
            config_kwargs = {
                "cache_mode": CacheMode.BYPASS,
                "word_count_threshold": 0,
                "excluded_tags": [],
                "remove_forms": False,
                "remove_overlay_elements": False,
                "css_selector": params.css_selector or 'body',
            }
            
            # æ ¹æ®æå–ç±»å‹é…ç½®ç­–ç•¥
            if params.extraction_type == 'css':
                extraction_strategy = JsonCssExtractionStrategy(
                    schema=schema  # ä½¿ç”¨ä¿®å¤åçš„schema
                )
                config_kwargs["extraction_strategy"] = extraction_strategy
                
            elif params.extraction_type == 'llm':
                logger.warning("LLM æå–æ¨¡å¼éœ€è¦ä¸€ä¸ªæœ‰æ•ˆçš„LLMå®ä¾‹ï¼Œå½“å‰ä¸ºé€»è¾‘å ä½ã€‚")
                extraction_strategy = LLMExtractionStrategy(
                    schema=schema,
                    instruction=params.prompt or "Extract structured data from the content",
                    llm=None
                )
                config_kwargs["extraction_strategy"] = extraction_strategy
            
            config = CrawlerRunConfig(**config_kwargs)
            
            result = await self._execute_with_timeout(
                crawler.arun(url=params.url, config=config),
                timeout=120
            )
            
            if not result.success or not hasattr(result, 'extracted_content') or not result.extracted_content:
                error_message = result.error_message or "æœªèƒ½æå–åˆ°ä»»ä½•ç»“æ„åŒ–å†…å®¹ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºé¡µé¢å†…å®¹æ˜¯åŠ¨æ€åŠ è½½çš„ï¼Œæˆ–è€…æå–ç­–ç•¥ï¼ˆSchema/Selectorï¼‰ä¸é¡µé¢ç»“æ„ä¸åŒ¹é…ã€‚"
                logger.error(f"âŒ æ•°æ®æå–å¤±è´¥: {params.url} - {error_message}")
                return {"success": False, "error": f"æ•°æ®æå–å¤±è´¥: {error_message}", "memory_info": await self._get_system_memory_info()}
            
            extracted_data = {}
            if result.extracted_content:
                try:
                    extracted_data = json.loads(result.extracted_content)
                except (json.JSONDecodeError, TypeError):
                    extracted_data = result.extracted_content

            return {
                "success": True, "url": params.url, "extracted_data": extracted_data,
                "metadata": {"extraction_type": params.extraction_type, "success": True},
                "memory_info": await self._get_system_memory_info()
            }
            
        except asyncio.TimeoutError:
            return {"success": False, "error": "æ•°æ®æå–æ“ä½œè¶…æ—¶ï¼ˆ120ç§’ï¼‰", "memory_info": await self._get_system_memory_info()}
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æå–æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)}")
            return {"success": False, "error": f"æ•°æ®æå–æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)}", "memory_info": await self._get_system_memory_info()}
        finally:
            await self._cleanup_after_task()

    async def _export_pdf(self, params: PdfExportParams) -> Dict[str, Any]:
        """å¯¼å‡ºPDFä¸ºbase64"""
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {
                    "success": False, 
                    "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–",
                    "memory_info": await self._get_system_memory_info()
                }
            
            logger.info(f"ğŸ“„ å¯¼å‡ºPDF: {params.url}")
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                pdf=True
            )
            
            result = await self._execute_with_timeout(
                crawler.arun(url=params.url, config=config),
                timeout=120
            )
            
            if not result.success or not result.pdf:
                logger.error(f"âŒ PDFå¯¼å‡ºå¤±è´¥: {params.url}")
                return {
                    "success": False, 
                    "error": "PDFå¯¼å‡ºå¤±è´¥",
                    "memory_info": await self._get_system_memory_info()
                }
            
            if params.return_as_base64:
                pdf_base64 = base64.b64encode(result.pdf).decode('utf-8')
                return {
                    "success": True,
                    "url": params.url,
                    "pdf_data": pdf_base64,
                    "format": "base64",
                    "type": "application/pdf",
                    "size_bytes": len(result.pdf),
                    "message": "PDFæˆåŠŸå¯¼å‡ºä¸ºbase64å­—ç¬¦ä¸²",
                    "memory_info": await self._get_system_memory_info()
                }
            else:
                return {
                    "success": True,
                    "url": params.url,
                    "size_bytes": len(result.pdf),
                    "message": "PDFæ•°æ®ä»¥äºŒè¿›åˆ¶æ ¼å¼æä¾›",
                    "memory_info": await self._get_system_memory_info()
                }
        except asyncio.TimeoutError:
            return {
                "success": False, 
                "error": "PDFå¯¼å‡ºè¶…æ—¶ï¼ˆ120ç§’ï¼‰",
                "memory_info": await self._get_system_memory_info()
            }
        except Exception as e:
            logger.error(f"âŒ PDFå¯¼å‡ºé”™è¯¯: {str(e)}")
            if "browser" in str(e).lower() or "context" in str(e).lower() or "NoneType" in str(e):
                await self._handle_browser_crash(e)
            return {
                "success": False, 
                "error": f"PDFå¯¼å‡ºé”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }
        finally:
            await self._cleanup_after_task()

    async def _capture_screenshot(self, params: ScreenshotParams) -> Dict[str, Any]:
        """æ•è·æˆªå›¾ä¸ºbase64ï¼ˆå¸¦å‹ç¼©ï¼‰"""
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {
                    "success": False, 
                    "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–",
                    "memory_info": await self._get_system_memory_info()
                }
            
            logger.info(f"ğŸ“¸ æ•è·æˆªå›¾: {params.url}")
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                screenshot=True
            )
            
            result = await self._execute_with_timeout(
                crawler.arun(url=params.url, config=config),
                timeout=120
            )
            
            if not result.success or not result.screenshot:
                logger.error(f"âŒ æˆªå›¾æ•è·å¤±è´¥: {params.url}")
                return {
                    "success": False, 
                    "error": "æˆªå›¾æ•è·å¤±è´¥",
                    "memory_info": await self._get_system_memory_info()
                }
            
            # å‹ç¼©æˆªå›¾
            compressed_screenshot = self.compressor.compress_screenshot(
                result.screenshot,
                quality=params.quality,
                max_width=params.max_width,
                max_height=params.max_height
            )
            
            # è·å–å‹ç¼©ä¿¡æ¯
            original_info = self.compressor.get_screenshot_info(result.screenshot)
            compressed_info = self.compressor.get_screenshot_info(compressed_screenshot)
            
            if params.return_as_base64:
                return {
                    "success": True,
                    "url": params.url,
                    "screenshot_data": compressed_screenshot,
                    "format": "base64", 
                    "type": "image/jpeg",
                    "size_bytes": len(base64.b64decode(compressed_screenshot)),
                    "compression_info": {
                        "original": original_info,
                        "compressed": compressed_info
                    },
                    "message": "æˆªå›¾æˆåŠŸæ•è·å¹¶å‹ç¼©ä¸ºbase64å­—ç¬¦ä¸²",
                    "memory_info": await self._get_system_memory_info()
                }
            else:
                return {
                    "success": True,
                    "url": params.url,
                    "size_bytes": len(base64.b64decode(compressed_screenshot)),
                    "compression_info": {
                        "original": original_info,
                        "compressed": compressed_info
                    },
                    "message": "æˆªå›¾æ•°æ®ä»¥base64æ ¼å¼æä¾›",
                    "memory_info": await self._get_system_memory_info()
                }
        except asyncio.TimeoutError:
            return {
                "success": False, 
                "error": "æˆªå›¾æ•è·è¶…æ—¶ï¼ˆ120ç§’ï¼‰",
                "memory_info": await self._get_system_memory_info()
            }
        except Exception as e:
            logger.error(f"âŒ æˆªå›¾æ•è·é”™è¯¯: {str(e)}")
            if "browser" in str(e).lower() or "context" in str(e).lower() or "NoneType" in str(e):
                await self._handle_browser_crash(e)
            return {
                "success": False, 
                "error": f"æˆªå›¾æ•è·é”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }
        finally:
            await self._cleanup_after_task()

    async def execute(self, parameters: Crawl4AIInput) -> dict:
        """æ‰§è¡Œå·¥å…·çš„ä¸»è¦æ–¹æ³•"""
        try:
            mode = parameters.mode
            params = parameters.parameters

            logger.info(f"ğŸš€ æ‰§è¡Œ Crawl4AI æ¨¡å¼: {mode}")

            # ä»»åŠ¡è®¡æ•°å’Œå®šæœŸå¼ºåˆ¶æ¸…ç†
            self._task_count += 1

            # åªæœ‰åœ¨è¾¾åˆ°æ¸…ç†é—´éš”æ—¶æ‰æ‰§è¡Œå†…å­˜æ£€æŸ¥
            if self._task_count % self._cleanup_interval == 0:
                memory_ok = await self._check_memory_health()
                if not memory_ok:
                    logger.warning("âš ï¸ æ‰§è¡Œå‰å†…å­˜æ£€æŸ¥å¤±è´¥ï¼Œå…ˆæ‰§è¡Œæ¸…ç†")
                    await self._force_memory_cleanup()

            # ç¡®ä¿æµè§ˆå™¨å·²åˆå§‹åŒ–
            await self.initialize()

            if mode == 'scrape':
                validated_params = ScrapeParams(**params)
                result = await self._scrape_single_url(validated_params)
                
            elif mode == 'deep_crawl':
                validated_params = DeepCrawlParams(**params)
                result = await self._deep_crawl_website(validated_params)
                
            elif mode == 'batch_crawl':
                validated_params = BatchCrawlParams(**params)
                result = await self._batch_crawl_urls(validated_params)
                
            elif mode == 'extract':
                validated_params = ExtractParams(**params)
                result = await self._extract_structured_data(validated_params)
                
            elif mode == 'pdf_export':
                validated_params = PdfExportParams(**params)
                result = await self._export_pdf(validated_params)
                
            elif mode == 'screenshot':
                validated_params = ScreenshotParams(**params)
                result = await self._capture_screenshot(validated_params)
                
            else:
                logger.error(f"âŒ æ— æ•ˆçš„æ¨¡å¼è¯·æ±‚: {mode}")
                return {
                    "success": False, 
                    "error": f"æ— æ•ˆçš„æ¨¡å¼ '{mode}'.",
                    "memory_info": await self._get_system_memory_info()
                }

            return result

        except Exception as e:
            logger.error(f"âŒ Crawl4AI å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}")
            return {
                "success": False, 
                "error": f"å‘ç”Ÿé”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        async with self._browser_lock:
            if self.crawler:
                try:
                    logger.info("ğŸ”š å…³é—­ crawl4ai æµè§ˆå™¨å®ä¾‹...")
                    await self.crawler.__aexit__(None, None, None)
                    self.crawler = None
                    self._initialized = False
                    self._task_count = 0
                    self._browser_start_time = None
                    
                    collected = gc.collect()
                    logger.info(f"æœ€ç»ˆåƒåœ¾å›æ”¶é‡Šæ”¾äº† {collected} ä¸ªå¯¹è±¡")
                    
                    logger.info("âœ… crawl4ai æµè§ˆå™¨å®ä¾‹å…³é—­æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ å…³é—­ crawl4ai æµè§ˆå™¨æ—¶å‡ºé”™: {str(e)}")
                    self.crawler = None
                    self._initialized = False
                    self._task_count = 0
                    self._browser_start_time = None